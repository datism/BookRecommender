{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "from typing import Dict, Text\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"retrival\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = \"hdfs://localhost:9000/user/nhom7/book/data/\"\n",
    "bookFilePath = dataPath + \"BX-Books.csv\"\n",
    "ratingsFilePath = dataPath + \"BX-Book-Ratings.csv\"\n",
    "books_df = spark.read.options(inferSchema=\"true\", header=\"true\", delimiter=';').csv(bookFilePath)\n",
    "rating_df = spark.read.options(inferSchema=\"true\", header=\"true\", delimiter=';').csv(ratingsFilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_pd = rating_df.drop('Book-Rating').toPandas()\n",
    "book_pd = books_df.select('ISBN').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = tf.data.Dataset.from_tensor_slices(dict(rating_pd))\n",
    "books = tf.data.Dataset.from_tensor_slices(dict(book_pd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\dd\\.virtualenvs\\tf_rec-5MMI-Cvb\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "ratings = ratings.map(lambda x: {\n",
    "    \"isbn\": x[\"ISBN\"],\n",
    "    \"user_id\": x[\"User-ID\"],\n",
    "})\n",
    "\n",
    "books = books.map(lambda x: x[\"ISBN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(80_000)\n",
    "test = shuffled.skip(80_000).take(20_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_ISBN = books.batch(1_000)\n",
    "user_ids = ratings.batch(1_000_000).map(lambda x: x[\"user_id\"])\n",
    "\n",
    "unique_book_ISBN = np.unique(np.concatenate(list(book_ISBN)))\n",
    "unique_user_ids = np.unique(np.concatenate(list(user_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.IntegerLookup(\n",
    "      vocabulary=unique_user_ids, mask_token=None),\n",
    "  # We add an additional embedding to account for unknown tokens.\n",
    "  tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.StringLookup(\n",
    "      vocabulary=unique_book_ISBN, mask_token=None),\n",
    "  tf.keras.layers.Embedding(len(unique_book_ISBN) + 1, embedding_dimension)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = tfrs.metrics.FactorizedTopK(\n",
    "  candidates=books.batch(128).map(book_model)\n",
    ")\n",
    "\n",
    "task = tfrs.tasks.Retrieval(\n",
    "  metrics=metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrivalModel(tfrs.Model):\n",
    "\n",
    "  def __init__(self, user_model, book_model):\n",
    "    super().__init__()\n",
    "    self.book_model: tf.keras.Model = book_model\n",
    "    self.user_model: tf.keras.Model = user_model\n",
    "    self.task: tf.keras.layers.Layer = task\n",
    "\n",
    "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "    # We pick out the user features and pass them into the user model.\n",
    "    user_embeddings = self.user_model(features[\"user_id\"])\n",
    "    # And pick out the movie features and pass them into the movie model,\n",
    "    # getting embeddings back.\n",
    "    positive_book_embeddings = self.book_model(features[\"isbn\"])\n",
    "\n",
    "    # The task computes the loss and the metrics.\n",
    "    return self.task(user_embeddings, positive_book_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RetrivalModel(user_model, book_model)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_train = train.shuffle(100_000).batch(8192).cache()\n",
    "cached_test = test.batch(4096).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "10/10 [==============================] - 1417s 140s/step - factorized_top_k/top_1_categorical_accuracy: 0.0587 - factorized_top_k/top_5_categorical_accuracy: 0.0599 - factorized_top_k/top_10_categorical_accuracy: 0.0606 - factorized_top_k/top_50_categorical_accuracy: 0.0634 - factorized_top_k/top_100_categorical_accuracy: 0.0659 - loss: 69998.2507 - regularization_loss: 0.0000e+00 - total_loss: 69998.2507  \n",
      "Epoch 2/3\n",
      "10/10 [==============================] - 1263s 126s/step - factorized_top_k/top_1_categorical_accuracy: 0.0798 - factorized_top_k/top_5_categorical_accuracy: 0.0858 - factorized_top_k/top_10_categorical_accuracy: 0.0886 - factorized_top_k/top_50_categorical_accuracy: 0.1001 - factorized_top_k/top_100_categorical_accuracy: 0.1092 - loss: 68684.8509 - regularization_loss: 0.0000e+00 - total_loss: 68684.8509\n",
      "Epoch 3/3\n",
      "10/10 [==============================] - 1203s 120s/step - factorized_top_k/top_1_categorical_accuracy: 0.0640 - factorized_top_k/top_5_categorical_accuracy: 0.0882 - factorized_top_k/top_10_categorical_accuracy: 0.0938 - factorized_top_k/top_50_categorical_accuracy: 0.1155 - factorized_top_k/top_100_categorical_accuracy: 0.1325 - loss: 65486.6044 - regularization_loss: 0.0000e+00 - total_loss: 65486.6044\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16a8898ba30>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(cached_train, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 355s 70s/step - factorized_top_k/top_1_categorical_accuracy: 0.0108 - factorized_top_k/top_5_categorical_accuracy: 0.0420 - factorized_top_k/top_10_categorical_accuracy: 0.0432 - factorized_top_k/top_50_categorical_accuracy: 0.0485 - factorized_top_k/top_100_categorical_accuracy: 0.0526 - loss: 32628.8073 - regularization_loss: 0.0000e+00 - total_loss: 32628.8073\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'factorized_top_k/top_1_categorical_accuracy': 0.01080000028014183,\n",
       " 'factorized_top_k/top_5_categorical_accuracy': 0.041999999433755875,\n",
       " 'factorized_top_k/top_10_categorical_accuracy': 0.04320000112056732,\n",
       " 'factorized_top_k/top_50_categorical_accuracy': 0.04854999855160713,\n",
       " 'factorized_top_k/top_100_categorical_accuracy': 0.05260000005364418,\n",
       " 'loss': 29663.412109375,\n",
       " 'regularization_loss': 0,\n",
       " 'total_loss': 29663.412109375}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(cached_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'0552998486' b'0971880107' b'0099771519' b'8838918600' b'8807810778'\n",
      " b'880781210X' b'884590184X' b'0552996009' b'0743222229' b'8806142100'\n",
      " b'848300268X' b'0349101779' b'0312422156' b'0330244078' b'0446355569'\n",
      " b'0140071814' b'0590660543' b'1853260134' b'8432210900' b'0330331744'], shape=(20,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# Create a model that takes in raw query features, and\n",
    "bruteforce = tfrs.layers.factorized_top_k.BruteForce(model.user_model, k=20)\n",
    "# recommends movies out of the entire movies dataset.\n",
    "bruteforce.index_from_dataset(\n",
    "  tf.data.Dataset.zip((books.batch(128), books.batch(128).map(model.book_model)))\n",
    ")\n",
    "\n",
    "# Get recommendations.\n",
    "_, titles = bruteforce(tf.constant([12]))\n",
    "# \n",
    "\n",
    "for t in titles:\n",
    "  print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_with_exclusions while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/retrival_model/1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/retrival_model/1/assets\n"
     ]
    }
   ],
   "source": [
    "path = (\"./model/retrival_model/1/\")\n",
    "tf.saved_model.save(\n",
    "    bruteforce,\n",
    "    path\n",
    ")\n",
    "\n",
    "loaded = tf.saved_model.load(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_rec-5MMI-Cvb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "77e106df7f450c66566a7610a3718f206c95a104637392e72bdeaab378155cd7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
